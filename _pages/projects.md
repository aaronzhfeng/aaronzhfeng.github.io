---
layout: single
title: "Projects"
permalink: /projects/
author_profile: true
---

## Publications

- <strong>ARCMEMO: ABSTRACT REASONING COMPOSITION WITH LIFELONG LLM MEMORY</strong><br>
  <a href="https://matt-seb-ho.github.io/">Matthew Ho</a>, <a href="https://www.linkedin.com/in/chen-si-780648248/">Chen Si</a>, <strong>Zhaoxiang Feng</strong>, <a href="https://yu-fangxu.github.io/">Fangxu Yu</a>, <a href="https://www.linkedin.com/in/yichi-yang/">Yichi Yang</a>, <a href="https://zhijianliu.com/">Zhijian Liu</a>, <a href="https://zhiting.ucsd.edu/">Zhiting Hu</a>, <a href="https://lianhui.ucsd.edu/index.html">Lianhui Qin</a><br>
  <a href="#">website</a> / <a href="https://arxiv.org/pdf/2509.04439">arxiv</a> / <a href="https://github.com/matt-seb-ho/arc_memo">code</a>

## Projects

<p>
  <img src="/assets/images/projects/CSE_151B.png" alt="Deep Learning for Climate Emulation" class="align-left" width="160">
  <strong>Deep Learning for Climate Emulation</strong><br>
  We build U‑Net‑based emulators for monthly global climate fields (tas, pr) and benchmark against CNN/ResNet/FNO. We show that skip‑connected encoder–decoders capture large‑scale patterns and reduce RMSE on 48×72 ClimateBench grids, while revealing underestimation of extreme precipitation.<br>
  <a href="#">website</a> / <a href="/assets/pdfs/projects/CSE_151B_Final_Report.pdf">report</a> / <a href="https://github.com/aaronzhfeng/DL_for_Climate_Emulation/">code</a>
</p>
<div style="clear: both;"></div>

<p>
  <img src="/assets/images/projects/CSE_156.png" alt="Efficient Expert‑Guided Retrieval" class="align-left" width="160">
  <strong>Efficient Expert‑Guided Retrieval</strong><br>
  We introduce a lightweight single‑block Mixture‑of‑Experts module for TinyBERT to enhance dense retrieval under tight compute. We find consistent gains in NDCG@10/Recall on BEIR SciFact while preserving efficiency, and we compare against fuller MoE variants to characterize the accuracy–efficiency trade‑off.<br>
  <a href="#">website</a> / <a href="/assets/pdfs/projects/CSE_156_Final_Report.pdf">report</a> / <a href="#">code (TBD)</a>
</p>
<div style="clear: both;"></div>

<p>
  <img src="/assets/images/projects/CSE_158.png" alt="Fake News Detection with C‑LSTM" class="align-left" width="160">
  <strong>Fake News Detection with C‑LSTM</strong><br>
  We build a CNN‑LSTM text classifier with pretrained embeddings and compare it to TF‑IDF/logistic baselines. We find strong in‑domain accuracy and diagnose domain‑shift failure modes, motivating regularization and robustness analysis for deployment.<br>
  <a href="#">website</a> / <a href="/assets/pdfs/projects/CSE_158_Final_Report.pdf">report</a> / <a href="#">code (TBD)</a>
</p>
<div style="clear: both;"></div>

<p>
  <img src="/assets/images/projects/ECE_176.png" alt="3D‑Enhanced Graph Reaction Prediction" class="align-left" width="160">
  <strong>3D‑Enhanced Graph Reaction Prediction</strong><br>
  We introduce a 3D‑aware reaction model by pre‑training 2D/3D MPNNs with an InfoNCE objective (QM9) and fine‑tuning a 2D encoder + LSTM decoder on ORD. We find that injecting 3D geometry improves representations for stereochemically constrained reactions under compute limits.<br>
  <a href="#">website</a> / <a href="/assets/pdfs/projects/ECE_176_Final_Report.pdf">report</a> / <a href="#">code (TBD)</a>
</p>
<div style="clear: both;"></div>

<p>
  <img src="/assets/images/projects/Math_111A.png" alt="ChemTransformer" class="align-left" width="160">
  <strong>ChemTransformer</strong><br>
  We build a transformer seq2seq system for ORD reaction prediction, exploring SMILES vs SELFIES and positional encoding designs. We find that traditional token‑level positional encoding outperforms compound‑level variants, while SELFIES improves validity with competitive token accuracy.<br>
  <a href="#">website</a> / <a href="/assets/pdfs/projects/Math_111A_Final_Report.pdf">report</a> / <a href="#">code (TBD)</a>
</p>
<div style="clear: both;"></div>

<p>
  <img src="/assets/images/projects/DSC_190_huzhiting.png" alt="Hybrid Financial Forecasting" class="align-left" width="160">
  <strong>Hybrid Financial Forecasting</strong><br>
  We investigate ARIMA/ARIMAX/SARIMA/GARCH baselines and build an ARIMAX+LSTM residual hybrid for NASDAQ daily returns (2010–2024). We find that a well‑tuned ARIMAX already captures most predictable structure, with hybrids yielding marginal gains in this data regime.<br>
  <a href="#">website</a> / <a href="/assets/pdfs/projects/DSC_190_Final_Report.pdf">report</a> / <a href="https://github.com/aaronzhfeng/AI_TimeSeries_Forecasting">code</a>
</p>
<div style="clear: both;"></div>
