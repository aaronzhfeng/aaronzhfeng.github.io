---
layout: single
title: "Projects"
permalink: /projects/
author_profile: true
---

## Publications

- <strong>ARCMEMO: ABSTRACT REASONING COMPOSITION WITH LIFELONG LLM MEMORY</strong><br>
  <a href="https://matt-seb-ho.github.io/">Matthew Ho</a>, <a href="https://www.linkedin.com/in/chen-si-780648248/">Chen Si</a>, <strong>Zhaoxiang Feng</strong>, <a href="https://yu-fangxu.github.io/">Fangxu Yu</a>, <a href="https://www.linkedin.com/in/yichi-yang/">Yichi Yang</a>, <a href="https://zhijianliu.com/">Zhijian Liu</a>, <a href="https://zhiting.ucsd.edu/">Zhiting Hu</a>, <a href="https://lianhui.ucsd.edu/index.html">Lianhui Qin</a><br>
  Preprint. [arXiv (PDF)](https://arxiv.org/pdf/2509.04439) · [Code](https://github.com/matt-seb-ho/arc_memo)

## Projects

- <strong>Deep Learning for Climate Emulation</strong> — We build U‑Net‑based emulators for monthly global climate fields (tas, pr) and benchmark against CNN/ResNet/FNO. We show that skip‑connected encoder–decoders capture large‑scale patterns and reduce RMSE on 48×72 ClimateBench grids, while revealing underestimation of extreme precipitation. (Report: CSE_151B_Final_Report.pdf)

- <strong>Efficient Expert‑Guided Retrieval</strong> — We introduce a lightweight single‑block Mixture‑of‑Experts module for TinyBERT to enhance dense retrieval under tight compute. We find consistent gains in NDCG@10/Recall on BEIR SciFact while preserving efficiency, and we compare against fuller MoE variants to characterize the accuracy–efficiency trade‑off. (Report: CSE_156_Final_Report.pdf)

- <strong>Fake News Detection with C‑LSTM</strong> — We build a CNN‑LSTM text classifier with pretrained embeddings and compare it to TF‑IDF/logistic baselines. We find strong in‑domain accuracy and diagnose domain‑shift failure modes, motivating regularization and robustness analysis for deployment. (Report: CSE_158_Final_Report.pdf)

- <strong>3D‑Enhanced Graph Reaction Prediction</strong> — We introduce a 3D‑aware reaction model by pre‑training 2D/3D MPNNs with an InfoNCE objective (QM9) and fine‑tuning a 2D encoder + LSTM decoder on ORD. We find that injecting 3D geometry improves representations for stereochemically constrained reactions under compute limits. (Report: ECE_176_Final_Report.pdf)

- <strong>ChemTransformer</strong> — We build a transformer seq2seq system for ORD reaction prediction, exploring SMILES vs SELFIES and positional encoding designs. We find that traditional token‑level positional encoding outperforms compound‑level variants, while SELFIES improves validity with competitive token accuracy. (Report: Math_111A_Final_Report.pdf)

- <strong>Hybrid Financial Forecasting</strong> — We investigate ARIMA/ARIMAX/SARIMA/GARCH baselines and build an ARIMAX+LSTM residual hybrid for NASDAQ daily returns (2010–2024). We find that a well‑tuned ARIMAX already captures most predictable structure, with hybrids yielding marginal gains in this data regime. (Report: DSC_190_Final_Report.pdf)

Notes:
- Publication entry lists as preprint and links to code per your request. The style mirrors the example layout with bold title, author list, and link row. You can later add venue/year once finalized.
